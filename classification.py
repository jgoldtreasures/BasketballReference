from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_scorefrom sklearn.linear_model import LogisticRegressionfrom sklearn.naive_bayes import GaussianNBfrom sklearn.metrics import classification_report, confusion_matrix, precision_score, mean_absolute_error, mean_squared_errorfrom sklearn import svmimport numpy as npimport pandas as pddef pretty_print(matrix):    pd.set_option('display.max_rows', None)    pd.set_option('display.max_columns', None)    pd.set_option('display.width', None)    pd.set_option('display.max_colwidth', None)    print(pd.DataFrame(matrix))def log_reg():    df = pd.read_csv('data/career_stats11.csv')    training = df[df['eligible'] == 1]    testing = df[df['eligible'] == 0]    # pretty_print(training)    # xtrain, xtest, ytrain, ytest = train_test_split(training.drop(columns=['Player', 'Hall of Fame', 'hof_prob'], axis=1), training['Hall of Fame'], test_size=0.20)    # logmodel = LogisticRegression()    # logmodel.fit(xtrain, ytrain)    # predictions = logmodel.predict(xtest)    #    # print(classification_report(ytest, predictions))    # print(confusion_matrix(ytest, predictions))    training_prob = training[['Player', 'hof_prob', 'Hall of Fame']]    logreg = LogisticRegression()    ypred = cross_val_predict(logreg, training.drop(columns=['Player', 'Hall of Fame', 'hof_prob'], axis=1),                              training['Hall of Fame'], cv=5, method='predict_proba')    ypred1 = cross_val_predict(GaussianNB(), training.drop(columns=['Player', 'Hall of Fame', 'hof_prob'], axis=1),                               training['Hall of Fame'], cv=5, method='predict_proba')    # print(ypred[:, 0])    hofprob = training_prob['hof_prob'] / 100    training_prob['hof_prob'] = hofprob    lreg = np.round(ypred, 6)    nbayes = np.round(ypred1, 6)    training_prob['log_reg'] = lreg[:, 1]    training_prob['naive_bayes'] = nbayes[:, 1]    training_prob = training_prob[['Player', 'hof_prob', 'log_reg', 'naive_bayes', 'Hall of Fame']]    training_prob = training_prob.sort_values(ascending=False, by=['hof_prob'])    pretty_print(training_prob)    # print(confusion_matrix(training_prob['Hall of Fame'], training_prob['log_reg']))    # print(precision_score(training_prob['Hall of Fame'], training_prob['log_reg']))    print(mean_absolute_error(training_prob['Hall of Fame'], training_prob['hof_prob']))    print(mean_absolute_error(training_prob['Hall of Fame'], training_prob['log_reg']))    print(mean_absolute_error(training_prob['Hall of Fame'], training_prob['naive_bayes']))    print(mean_squared_error(training_prob['Hall of Fame'], training_prob['hof_prob']))    print(mean_squared_error(training_prob['Hall of Fame'], training_prob['log_reg']))    print(mean_squared_error(training_prob['Hall of Fame'], training_prob['naive_bayes']))def log_reg1():    df = pd.read_csv('data/career_stats11.csv')    training = df[df['eligible'] == 1]    training_prob = training[['Player', 'hof_prob', 'Hall of Fame']]    logreg = LogisticRegression()    ypred = cross_val_predict(logreg, training.drop(columns=['Player', 'Hall of Fame', 'hof_prob'], axis=1),                              training['Hall of Fame'], cv=5)    ypred1 = cross_val_predict(GaussianNB(), training.drop(columns=['Player', 'Hall of Fame', 'hof_prob'], axis=1),                               training['Hall of Fame'], cv=5)    hofprob = training_prob['hof_prob'] / 100    training_prob['hof_prob'] = hofprob    training_prob['log_reg'] = ypred    training_prob['naive_bayes'] = ypred1    training_prob = training_prob[['Player', 'hof_prob', 'log_reg', 'naive_bayes', 'Hall of Fame']]    training_prob = training_prob.sort_values(ascending=False, by=['hof_prob'])    pretty_print(training_prob)    print(confusion_matrix(training_prob['Hall of Fame'], training_prob['log_reg']))    print(confusion_matrix(training_prob['Hall of Fame'], training_prob['naive_bayes']))    print(precision_score(training_prob['Hall of Fame'], training_prob['log_reg']))    print(precision_score(training_prob['Hall of Fame'], training_prob['naive_bayes']))def support_vector():    df = pd.read_csv('data/career_stats10.csv')    training = df[df['eligible'] == 1]    testing = df[df['eligible'] == 0]def squash():    df = pd.read_csv('data/career_stats10.csv')    df2 = pd.DataFrame.copy(df)    # df2['leaders'] = df[]    columns = ['Scoring Champ', 'AST Champ', 'TRB Champ', 'STL Champ', 'BLK Champ']    df2['leaders'] = sum([df2[i] for i in columns])    df2 = df2.drop(columns, axis=1)    df2.to_csv('data/career_stats11.csv', index=False)def main():    log_reg1()    # squash()main()